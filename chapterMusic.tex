%% ------------------------------------------------------------------------- %%
\chapter{Mobile Music Overview}
\label{cap:mobilemusic}

 \epigraph{
	Let the music in tonight\\
	Just turn on the music\\
	Let the music of your life\\
	Give life back to music.}
 {Daft Punk}

\paragraphdesc{the first days of mobile music using cellphones and ringtones}
The idea of using mobile devices in art emerged with the development of mobile technologies during the 1990's~\citep{deSouzaeSilva2004}.
The first experiences with mobile music date back to the beginning of the 2000's, when cellphones became mobile, acquired custom ringtones and began to come supplied with long-life batteries.
At this time, mobile networks were mostly be used for telephone calls, but this was enough to awaken creativity in music.
Although the term had first been theorized as ``\textit{musica mobilis}'' during the 1980's in a discussion regarding the Walkman~\citep[p.~5--6]{Gopinath2014handbookmobilemusicstudies1}, those recent events characterized the beginning of a new research field called Mobile Music~(MM).

\paragraphdesc{discussion about Dialtones}
Some authors consider the ``Dialtones''~\citep{Levin2001} music concert as the first MM public concert~\citep{Wang2014ocarina,Weinberg2005interconnected}.
At this event, the audience was invited to download a ringtone and sit in an assigned seat in the theater before the concert started.
The performer then called specific phone numbers at predetermined times during the performance using a computer program to create the music.
This type of concert seems to avoid user interaction, as the audience participates only passively, but it did inspire many other MM experiences henceforward.

\paragraphdesc{the list of cellphone performances from 2000 to 2004}
In the early 2000's, the popularity of mobile devices was increasing and many music experiences using them were being conceived.
\citeauthor{Levin2004}, author of Dialtones, decided to register many experiences and experiments from that time in an informal catalog~\citep{Levin2004}.
Examples from that list include suspending 1,000 smartphones (``\textit{Handywolke}'' by Peter Hrubesch and Dirk Scherkowski) and illuminating helium balloons based on electromagnetic fields affected during calls (``\textit{Sky Ear}'' by Usman Haque).
The innovative creativity of MM was then publicized to lay audiences through performances and scientific papers.

\paragraphdesc{MM got noticed in computer music conferences}
The computer music research field presents important annual conferences that have become popular venues in which to publish information about new MM experiences.
The conference on New Interfaces for Music Expression~(NIME) is one of the main events that considers and explores new methods to use mobile devices as interactive instruments or interfaces.
Aesthetic and musical aspects of MM are more commonly discussed at the International Computer Music Conference~(ICMC), while more computational outlooks are usually presented at the Sound and Music Computing Conference~(SMC).
Conferences such as the ACM Conference on Human Factors in Computing Systems~(CHI) and the ACM International Conference on Human-Computer Interaction with Mobile Devices and Services~(MobileHCI) are other venues that explore MM projects outside the computer music area.

Although there are many different conferences, topics overlap among them, and I will partition the discussions in the following thematic axes: interventions at determined spaces; collaborations; systems for application development; new musical applications; reports; and books.
The papers discussed below are selected examples in MM that offer a more or less chronological account of the field on each of these thematic axes.

\paragraphdesc{interventions and collaboration at open spaces (city, theater, orchestras)}
An early intervention using MM is described by \cite{Gaye2003sonic} regarding the Sonic City project.
Users were able to create electronic music while walking on the streets with a portable kit that included a small laptop.
\cite{Tanaka2004mobile} discussed a system for collaborative music creation using Personal Digital Assistants~(PDA) at NIME 2004, which applied the idea of real-time single stream composition.
The results of the first mobile phone orchestra, which took advantage of some features of the Nokia N95 smartphone, were presented during the ICMC 2008, and included a review of many past works on MM~\citep{Wang2008domobilephones}.
At SMC 2009, \citeauthor{Tahiroglu2009towards} presented a system for audience participation using mobile devices and discussed the way in which it extended mobile phone orchestras~\citep{Tahiroglu2009towards}.

\paragraphdesc{systems for app development and new musical apps}
There are several examples in the literature that consider systems for application development and applications made for music creation.
At ICMC 2003, a port of Pure Data for the PocketPC platform was presented~\citep{Geiger2003pda}, while
\cite{Schiemer2005pocketgamelan} presented a way to convert Pure Data patches to J2ME applications created during the Pocket Gamelan project at NIME 2005.
At ICMC 2007, \cite{Essl2007shamus} discussed ShaMus, an application that makes music using mobile sensors, such as accelerometers and magnetometers.
At SMC 2013, \cite{Ekeus2013predictable} presented an application that allowed the user to create musical patterns with the help of artificial intelligence based on Markov chains.
At the same conference, \cite{Baldan2013melody} discussed an application to help children develop rhythm and timing abilities.
At SMC 2015, a mobile application for multimedia interactions using common inputs like sensors and buttons that processed video input in real time for interaction with low-level features was described~\citep{Krekovic2015sound} --- something feasible nowadays due to the advances in mobile device technologies.

\paragraphdesc{reports describing the state of art}
One of the first reports on MM was presented at NIME 2006 by \cite{Gaye2006report}, describing the current field, the emerging community, and many past events.
An update of this report is found in a paper presented at NIME 2013, that evaluated 10 years of MM projects and papers presented at NIME~\citep{John2013updating}.
The publication of The Oxford Handbook of Mobile Music (in two volumes), which includes a bibliography related to the research field, can be considered as the most complete publication regarding the use of mobile technology and the social aspects of its acceptance~\citep{Gopinath2014handbookmobilemusicstudies1,Gopinath2014handbookmobilemusicstudies2}.

\paragraphdesc{mobile music discussed in the media and book}
The appearance of MM news in the media is also worth mentioning.
A New York Times article dated 5 December 2009, ``\textit{From Pocket to Stage, Music in the Key of iPhone}'', discusses mobile phone orchestras from Stanford University and Michigan University and the John Hollenbeck Large Ensemble, a big band using smartphones as instruments~\citep{Miller2009nytimes}.
``\textit{Gorillaz give away their new album made on an iPad}'' is the title of a post in The Guardian (25 December 2010) describing an album composed using twenty mobile apps and instruments~\citep{Smith2010gorillaz}.
Bjork's ``\textit{Biophillia}'' album and mobile app appeared in a post on 13 June 2014 entitled, ``\textit{iPad art by singer Bjork becomes first-ever app in MoMA's permanent collection}'', became another important landmark in the media while described as \textit{part album, part interactive multimedia presentation}~\citep{Hughes2014bjork}.

\paragraphdesc{the evolution of smartphones (technologies) during 2000's}
Smartphones, the improved versions of the original cell phones, are now taking the place of computers.
Mobile processor power increases at the same time miniaturization reduces the size of many components, and batteries last longer than before even under this condition.
They usually utilize many diverse technologies into a single smartphone board: as of 2010, several simultaneous communication technologies exist, many processing cores, large storage capacity, and dozens of sensors~\footnote{
	Regarding technological upgrades throughout the years, the reader may find that, in the past 10 years, processors got almost 10 times faster in each core, memory capacity got 25 times bigger, and communication technologies increased the number of network bands and their bandwidth~(Appendix~\ref{ape:gsmarena-n95-s7}).}.
It is interesting to note that by 2006, one could find smartphones with WLAN, 3G, Bluetooth, infrared, GPS and radio technologies integrated into a single board~\footnote{
	Appendix~\ref{ape:android-sensors} shows sensors available for mobile devices, both base sensors (the ones related to physical sensors) and composite sensors (which build upon values from base sensors, or merge two or more sensors).}.
These technologies make this device a suitable option for collaborative and interactive ideas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background: Network Music}\index{network music!history}
\label{sec:networkmusic}

\paragraphdesc{network music definition}
As soon as 19th century technologies became capable of reproducing and playing back audio, even before the digital age, communication technologies inspired musical and artistic experiences.
Audio transmission and control between distant places through telephones and radios were just the beginning, before the introduction of computers.
In this context, network music can be defined as music that takes advantage of network technologies to be composed, played, or simply exist.
The term is common in academic publications and also used as computer network music, although the music based on using computer networks lacks universal terminology~\citep{Akkermann2014computer}.

\paragraphdesc{network music as a mother field of mobile music}
Computers have been used for network music since the 1970's.
Switching computers to phones for concerts during the 1990's made network music a possible mother field of MM, or at least the most important predecessor.
Smartphones appeared after cellphones and included technologies such as advanced operating systems and browsers for Internet access.
Smartphones can thus be used the same way computers can be, including the composition and sharing of network music, but with the added convenience of mobility.

\paragraphdesc{local and distributed network characteristics}
One of the first setting decisions for network music consists of determining whether the composition would work for local networks or through the Internet, or even both cases.
Note that local networks are mostly associated with a local area network (LAN), but this setup can have computers located far away from each other.
Musical performances using LAN in the context of a dozen meter radius might take advantage of small packet loss and fast packet exchange.
On the other hand, performances using the Internet or wide area networks (WAN) can have users from different networks.
Additionally, network music experiences have also been using other technologies, like radio networks and telephone lines, to extend the possibilities of interaction, as we will see below.

\paragraphdesc{early experiences: john cage - radio music}
John Cage was one of the first composers to realize the potential of using electronic technologies to improve musical experiences~\citep[p.~16-17]{Weinberg2003}.
Cage's ``\textit{Imaginary Landscape No. 4}'' can be considered the first \textit{electronic interdependent musical network} as it was composed for 12 radios played by 24 artists.
In this composition, the radio network was the source of sound, and the performers were guided by a score to select specific frequencies one at a time~\citep[p.~24-25]{Weinberg2003}. 

\paragraphdesc{league of automatic music composers and the hub}
Musicians John Bischoff, Jim Horton, and Rich Gold created The League of Automatic Music Composers in 1977, taking advantage of the arrival of the personal computer, particularly the \textit{Comodore KIM-1}.
The League was the first group to incorporate this technology into musical performances.
Each member was able to send and receive musical data through the network created between them~\citep[p.~29]{Weinberg2003}.
In the mid 80's, the League evolved into The Hub group, which had more precise communication schemes using central computers and the MIDI protocol to facilitate the interaction.
They also tried remote collaboration through telephone lines during this time.

\paragraphdesc{Wiencouver IV - Telephone Music}
From 1979 to 1983, a series of festivals took place in the imaginary city of Wiencouver, located metaphorically between Vienna and Vancouver, with events simultaneously in both cities.
In its fourth edition, the \textit{Wiencouver IV} in 1983, the festival organizers presented the project \textit{TelefonMusik}, in which performances were transmitted through telephone lines before being amplified to the public~\citep[p.~19-20]{Corby2013}.
\textit{Wiencouver IV} is the last one in its series and, with the \textit{Ars Electronica} festival, it represents events in which the technology could be explored for collaborative performances at that time also using slow-scan television and telefacsimile~\citep[p.~146-47]{Gere2006art}. 

\paragraphdesc{TransMIDI e FMOL}
Using an ATM-based LAN and SGI machines running IRIX, \cite{Gang1997transmidi} implemented and tested the prototype of a TransMIDI system so that users could join network sessions and share MIDI messages asynchronously.
This solution applied Multicast groups in order to allow cooperative networking applications with many layouts and topologies~\citep{Gang1997transmidi}.
For another project, the researcher \citefullauthor{Jorda1999fmol} was invited to create the FMOL project in 1997 for collective composition.
Since 1989, he has conceived virtual instruments and interactive systems and built a tool that would suit musicians and hobbyists with a wide range of complexity and creativity.
This system required a computer with any 16-bit multimedia sound card and a 16-MB Pentium 100 MHz with Windows 95 and the Microsoft DirectX libraries that allowed up to four composers for real-time synthesis, composition and control~\citep{Jorda1999fmol}.

\paragraphdesc{laptop orchestras}
In the next decade, the popularization of laptops inspired a new type of ensemble usually referred to as \textit{laptop orchestras}.
In these arrangements, it is possible to interact through music and chats, projections, code and file sharing over a network~\citep{Gasperini2011laptop}.
Most of these orchestras were founded inside academic environments, such as Princeton University, Stanford University, Georgia Tech, Virginia Tech, and Louisiana State University.
Other bands, such as Benoît and the Mandelbrots, perform on stage using only laptops at the site of \textit{International Association of Laptop Orchestras}~\citep{IALO2016laptop}.

Performances with laptops caught audiences' attention due to its innovative setup, but it also has a drawback: during one performance one critic insinuated that the orchestra's performers were checking their emails and it was affirmed that musicians and the music were disconnected regarding physical expression~\citep{Trueman2007}.

Interaction with traditional musicians is another achievement of these orchestras and seen with laptops generating musical notation for musicians holding acoustic instruments and communicating through the network~\citep{Lee2013mixed}.
In fact, an extensive environment can be achieved when using computer networks for laptop interactions with traditional ensembles.

\paragraphdesc{theoretical discussion}
Network music experiences and technological advances opened up many possibilities for musicians and scientists.
This technological evolution inspired researchers to evaluate and theorize about network music.
The works cited below are important papers that present great discussions, paradigms, insights and numbers regarding network music in Brazil and around the world.

\paragraphdesc{kon iazzeta: internet music / lago e kon: quest of latency}
Although little research was available about internet music conception from musical and technological perspectives 20 years ago, the cutting edge technology and its rapid progress were setting the stage for internet music to soon become a real or virtual reality~\citep{Kon1998internetmusic}.
Audio compression or synthesized music processes would need to improve to diminish delays inherent in interacting through the Internet and allow interactions between musicians and their audience in real time.
Expected solutions mentioned in the paper are now a reality, including technologies such as IPv6, gigabit Internet connections, and super computers.
The authors conclude by suggesting three factors that need attention for internet music compositions: the pulse, the tempo, and the style.
They also emphasize that the composer will need to compensate for technological limitations throughout their own composition~\cite{Kon1998internetmusic}.

\paragraphdesc{tanzi philosophical discussion}
Paradigms related to music and networks are discussed in terms of aspects like performance, convergence, invention, subjects, plurality, and many other characteristics intrinsic to network music~\citep{Tanzi2001observations}.
\citeauthor{Tanzi2001observations} presents network music structures from different points of view, including that of the composer, performer, audience and listener, that can be situated at different times in different spaces from the performance perspective where existence on the Internet is much like reality in some cases and virtual in others, correlating this discussion with the previous ideas~\citep{Kon1998internetmusic}.

\paragraphdesc{music perception surveys, books, papers}
Human perception has improved through our centuries of sound experiences~\citep{Jourdain1997music}.
Following this idea, network music deserves some attention regarding its effects on audiences and participants.
The effects of latency and jitter have been studied and surveyed for the past focusing mainly on perceived synchronization and they present parameters to bear in mind while proposing musical interactions.
In these works, the authors demonstrated that human perception is constrained by some limits of rhythm, haptics, and other limitations discovered and evaluated through many careful experiments, whereby latencies up to 30~ms are acceptable for multimedia applications, but numbers higher than that range can affect the interaction~\citep{Schuett2002effects,Lago2004thequest,Rottondi2016overview}.

\paragraphdesc{gil weinberg: music networks}
Many works using the Internet for music interactions were also grouped and discussed in works at the turn of the last century.
\cite{Weinberg2003} devised four categorizations: Server, Bridge, Shape, and Construction Kit based on participants' interconnection levels and the role of the computer in this environment.
Presented also as a basis for new ideas relating to network music, this research introduced the concept of Interconnected Musical Networks~\citep{Weinberg2005interconnected}. 
In these networks, a musical instrument can be played by more than one person simultaneously and many musicians can interact with the music system through any network system.
Although this concept was wide-ranging in its scope, it embraces the new network solutions as valuable elements for new musical instruments.

\paragraphdesc{alvaro barbosa: network systems for music}
\cite{Barbosa2003dispacedsoundscapes} theorized about network music with a focus on network systems and collaboration and proposed a classification diagram based mainly on musical interaction and its location.
The interaction methods are either synchronous or asynchronous, while the location can be co-located or remote and combining interaction methods and location, he found possible characteristics of collaborative systems from strict synchronized local performances to remote free improvisations~\citep{Barbosa2003dispacedsoundscapes}.
This mix also provided new sonic art creation paradigms toward network interaction.

\paragraphdesc{link with Marcio's master thesis, link with distance problems and new ideas}
The question of existence on the Internet and the possibilities of dealing with space and time constraints were technically scrutinized by following the technological advances of network communication and analyzing possible structures for network performances~\citep{Carot2007network,Carot2007networked,Carot2008distributed,Carot2009fundamentals,Carot2010low}.
Models based on specific approaches were defined, such as realistic interaction, master/slave, laid back, latency accepting, delayed feedback, and fake time, which are somewhat similar to \citefullauthor{Weinberg2003}'s and \citefullauthor{Barbosa2003dispacedsoundscapes}'s models, but focus on audio latency for each participant.
All of these works can be arranged together to better understand network music ideas and limitations. 

\paragraphdesc{distributed music projects}
In addition to those theoretical discussions, network music ideas have been put into practice frequently during the past decade.
The diffusion of the megabit Internet benefited distributed music projects, while providing fast communication between distant places.
The concept behind distributed music can be explained by the idea that when you distribute, there may be similar music in every place, but it is not necessarily the same.
By taking advantage of local networks or the Internet, researchers improved ideas discussed in the beginning of this section.
These improvements occurred around the world and also in Brazil, as we will see below.

\paragraphdesc{distributed live coding projects}
Laptop orchestras started to evaluate collaborative remote live coding into their performances.
\cite{Swift2014networked} used a network tunnel --- with Secure Shell (SSH) --- through a server in Australia to conduct a live coding performance that included performers situated in Germany and San Jose, California, USA.
\cite{Roberts2012gibberlivecoding} and \cite{Ogborn2014live} have been using the browser to share code among connected performers, with Gibber and Extempore, respectively.

\paragraphdesc{Mobile/NuSom research projects}
Simultaneously in Brazil, Mobile and NuSom research projects from the Universidade de São Paulo (USP) performed distributed music across countries as well.
NetConcerts from \citep{Arango2014thesis} evaluated musical collaboration between members of those projects from São Paulo, Brazil, and the Sonic Arts Research Centre (SARC), Queen's University Belfast, Northern Ireland.
Researchers discovered many Brazilian challenges in establishing a reliable communication at network music performances due to connectivity constraints such as delay and jitter~\citep{Arango2013challenges,Arango2014thesis}.
One of these researchers evaluated the tools used during these concerts and, comparing with available solutions, came up with the conception of JackTripMod as an alternative solution for home users aiming to perform through the Internet~\citep{Tomiyoshi2013thesis}.

\paragraphdesc{Compmus research projects}
In another project from USP researchers, the Medusa system was developed as a distributed music environment~\citep{Schiavoni2011medusa,Schiavoni2012network,Schiavoni2013network}.
This system permits users to share audio and MIDI data flow among connected computers using many network protocols and audio API's~\citep{Schiavoni2013thesis}.
In addition, I created SuperCopair as a tool for collaborative~\citep{deCarvalhoJunior2015supercopair} and cooperative live-coding~\citep{deCarvalhoJunior2015cooperative} through the Cloud that was developed in a personal partnership with members from the University of Michigan, Ann Arbor, USA.
Through this tool users can share and run codes that synthesize music in a local or distributed manner.
The focus is to make it easy for users to practice distributed music without many steps or technical knowledge.

\paragraphdesc{end of the section}
Network music changed musical concert paradigms beginning in the 1970's.
Many new technologies were used during this time to interconnect desktop computers, laptops, and supercomputers.
Introducing smartphones into this field provides mobility, easy access to sensors and touchscreens, and also focuses attention on wireless connection limitations and battery consumption.
The use of smartphones in network music and the development of MM are discussed in the next sections.

%% ------------------------------------------------------------------------- %%
\section{Mobile Devices overview}\index{mobile music!mobile controllers}
\label{sec:mobiletechnologies}

\paragraphdesc{mobile devices fast upgrades and patent warfare}
Mobile device systems are constantly upgraded around the world.
On one side, the market (e.g. Apple and Samsung) has a patent warfare resulting in discussions regarding who invented the new feature ``first'', while on the other side, users and researchers are updating their systems as soon as possible, while also creating open source alternatives.

Examples of MM open source alternatives for smartphones are the Android system, libpd library for Pure Data, Web Audio API, and accessories like Arduino connected through USB On-The-Go~(OTG).
Although only new devices are closely related to these new features, smartphones are becoming similar to desktop computers from a hardware plug-and-play perspective.

\paragraphdesc{mobile devices evolution in processing and computing}
Nowadays, the evolution in mobile devices' processing and computing capabilities follow a similar pace as personal computers, even though their purpose and usefulness are considerably different.
Mobile devices have been reduced in size, increased in power, saves energy, and efficiently dissipates heat with the benefit of continuous research that also improved other computing systems such as servers and desktop computers~\cite{Barroso2007energy}.

\paragraphdesc{combining mobile hardware into musical processes}
The mobile evolution also involves the possibility of combining other mobile hardware, such as cameras, communication interfaces, and sensors. 
These are features that appear from time to time, which, once available, attract the attention of performers and musicians due to their artistic affordances.
The musical use of such technology allows the inclusion of both sensors and network data into musical processes that can include gestural control and interaction among users.

\paragraphdesc{operating system evolution: performance for fast interaction}
Hardware advances have spurred on software advances as well.
The operating system~(OS) is one of the most important pieces of software on mobile devices.
Symbian OS and Palm OS were available on devices used in mobile music projects during the early 2000's, while iOS and Android acquired their places on the market afterwards.
Initially, mobile devices running iOS had an advantage against Android devices due to Apple's development of both hardware and OS.
Mobile devices running an Android system currently have low latency in new versions of the OS even with different hardware.
There is a low latency with digital sound processing (DSP) performance on Android devices when evaluating different devices and API versions~\citep{Bianchi2012ontheperformance}.

\paragraphdesc{operating system evolution: libraries for fast integration}
Following advances in OS, libraries, and languages, mobile devices have achieved success on audio synthesis, processing, and reproduction.
The portability of computational solutions from desktop computers to mobile devices has accelerated this process.
Most notable works include Pure Data to Pocket PCS~\citep{Geiger2003pda}; Pure Data patches compiled with J2ME~\citep{Schiemer2005pocketgamelan}; the development of libpd, a library to run Pure Data patches in many programming languages (C, C++, C\#, Objective C, Java, Python) and systems (Android and iOS)~\citep{Brinkmann2011embeddingpd}; Mobile STK~\citep{Essl2006mobilestk} ported from \citeauthor{Cook1999stk}`s STK~\citep{Cook1999stk}; the use of ChucK on the Ocarina app~\citep{Wang2008domobilephones} for iOS; the port of CSound for Android devices~\citep{Yi2012csound}; and the iSuperColliderToolKit developed for iOS~\citep{Ito2015isupercolliderkit}.
These works are further described in the next section.

Android device programmers can use low level native code for audio processing without any library to improve performance, and this alternative performs better than Java.
This comparison was evaluated in work conducted by me and other researchers~\citep{deCarvalhoJunior2013fftbenchmark} and is part of a master's  thesis~\citep{bianchi2014processamento}.
Although loading native code takes 1 to 4ms, the native code surpasses pure Java in most situations.
Moreover, the multi-thread version of native code has similar performance to the single-thread version due to Android threading policies that render multi-threading unnecessary.

\paragraphdesc{music interaction from browser}
The use of browsers for MM is another approach that needs attention~\citep{Wyse2013viability}.
From web pages to web applications, researchers  can create interactive projects that allow users to control audiovisual parameters during performances~\citep{Allison2013nexus,Weitzner2012massmobile}.
Further development of audio APIs currently allows browsers to process and synthesize audio inside webpages.
During the 1990's, web pages only played Wave and Au files and now they can play lossy and lossless formats, such as MP3 and FLACC, respectively.
Audio streaming has been an alternative since the beginning of the Internet.
\cite{Duckworth2005virtual} provides a detailed description of most of the initial concepts and works related to the use of music for the web.

Other audio resources on browsers include audio synthesis through Web Audio API with high performance and flexibility, even on mobile browsers.
Applications with Web Audio API are turning web pages into desktop software, like the interactive audio renderer~\citep{Matuszewski2016interactive} and DAWs~\citep{Kleimola2015daw}.
Web Audio libraries like Gibber~\citep{Roberts2012gibberlivecoding} and WAAX~\citep{Choi2013waax} have facilitated the use of music technologies on browsers by reducing the complexity of coding required and the interface design.
Moreover, the majority of new mobile devices can run Web Audio API applications through browsers, which makes it portable to many systems and platforms.

\paragraphdesc{direct manipulation: buttons and touch}
In terms of MM interaction, the applications can apply direct or indirect audio manipulation.
Apps using buttons, visual controls, and touchscreen permit direct manipulation as the interaction depends on a user's actions, i.e., the user decides when to send new inputs or parameters to the application through the interface.
The application ``Touches on The Line'', described in Appendix~\ref{apesec:apptouchesontheline}, applies the idea of direct manipulation.
The touch position acts as an input to the CSound synthesizer and sound is generated following the position of the fingers on the screen before being sent to other users who will reproduce the same audio~\citep{deCarvalhoJunior2013touches}.

\paragraphdesc{indirect manipulation: sensors and gestures}
Developers take advantage of sensors, gestures, and  multimedia resources like audio and camera for indirect audio manipulation on mobile applications.
Although these inputs sometimes require specific actions from the user's side to interact with MM, their current values are sent with a fixed (or variable) sample rate to the application.
This situation provides new parameters to the application constantly, even if the user is not interacting with the mobile device.
The application ``Sensors2PD'' described in Appendix~\ref{apesubsec:appsensors2pd} is an example of indirect manipulation where users can control which sensor will send values to a Pure Data patch, so sensor events are sent when they are acquired by the system~\citep{deCarvalhoJunior2014sensors2pd}.

\paragraphdesc{mixed manipulation: direct and indirect}
It is also possible to have mixed interaction when applications allow direct and indirect manipulation of audio parameters.
Ocarina~\citep{Wang2014ocarina} uses this in its approach whereby buttons manipulate the tone of the output while, at the same time, the audio input is used to capture the blow from a user's mouth to control the amplitude of that tone's synthesis.
The project ``Hoketus'' described in Appendix~\ref{apesec:apphoketus} uses available wireless network information to define the sound synthesizer, to modify synthesizer parameters through touch on a smartphone screen, and to allow the use of accelerometer values as parameters at the same time~\citep{Bandeira2014notes,deCarvalhoJunior2015indoor}.
The direct, indirect, and mixed manipulation definitions here are far from being a general classification, although it gives support to HCI concepts with similar descriptions.

\paragraphdesc{link mobile controllers to mobile instruments}
Given all of these technological advances and features, we will discuss the use of mobile devices, especially smartphones, as mobile instruments in the next section.
Problems faced by developers and musicians are presented with more detailed information regarding musical aspects.
Projects and companies that encouraged the boom of MM in recent years are cited as well as past distinguished researchers from the field.














%% ------------------------------------------------------------------------- %%
\section{Mobile Devices as Music Instruments}\index{mobile music!mobile instruments}
\label{sec:mobileinstruments}

\paragraphdesc{definition based on Miranda/Wanderley: new digital music instruments}
The insertion of mobile devices in the ambit of musical instruments has been an important approach for the digital musical instrument (DMI) concept~\citep{Miranda2006newdigitalmusical}.
The representation of a DMI runs from the input to the sound production, including a gestural controller, mapping section, primary feedback from the gestural controller, and secondary feedback from the sound production.
This possible approach to represent a DMI is similar to the process of generating sound on acoustic instruments.
However, the fixed causality is optional, so the same input can be mapped to a different sound~\citep{Miranda2006newdigitalmusical}.
The possible ways of using smartphones discussed in Section~\ref{sec:mobiletechnologies} can place them in any position of this representation without the necessity of being a complete DMI.

\paragraphdesc{the bottleneck: data processing, OS, programming languages}
Adopting mobile devices as musical instruments has some restrictions.
During musical performances and musical practices, real-time signal processing and network data streaming 
%are some of the bottlenecks that 
require attention.
%Current mobile devices have processing similarities with desktop computers, but there are still critical issues with some algorithms, e.g. any acoustic simulation with many objects, parameters, and variables.
Processing high quality audio on mobile devices in real-time while applying effects on many audio sources can result in undesirable glitches that also occur on any computer.
Despite that, companies are constantly improving device hardware and software, and offering new options for composers and performers.
The composers can also avoid some strategies until they become adept at running them on a possible device.

\paragraphdesc{programming languages and libraries for digital signal processing}
At this point, a recommended strategy to create musical instruments with mobile devices is to take advantage of computer music solutions imported from computers like Pure Data, STK, Chuck, CSound, and SuperCollider, and also the Web Audio API.
The migration of code and creativity from these solutions to mobile applications is thus accomplished without much rework.
Available computer music languages, libraries, and the Application Programming Interface~(API) take care of mobile constraints, settings, mobile resources (like sensors), and dealing with audio input and output even when specific  hardware for Digital Sound Processing~(DSP) is unavailable.
Additionally, research has also created toolkits and systems to facilitate the development of musical applications encapsulating language and library compatibilities in order to make the work easier for novices of MM technology.
As a tendency of using mobile devices as musical instruments, there were many MM projects using the solutions discussed  here.
These projects are presented below.

\paragraphdesc{mobile music examples with PD}
\citefullauthor{Puckette1997puredata}'s ``Pure Data'' language~\citep{Puckette1997puredata}, also known as Pd, has been integrated with many MM projects since the beginning of 2000.
The first integration was \citefullauthor{Geiger2003pda}'s port of Pd to Pocket PCs running Linux~\citep{Geiger2003pda}.
Using the PDa application created from this portability, the users would open and interact with patches through the touch screen.
The author also discusses the condition of mobile technologies at the time of the paper:

\begin{adjustwidth}{6 em}{0pt}
	``Having a new technology available to generate music is always a challenging experience, only the future will show what kind of application this will have'' ... ``PDa delivers a proof of concept, it shows one direction into which computer music systems might go in the future, probably similar to laptop computer music revolution we saw in the last few years.'' \cite[p.~4]{Geiger2003pda} 
\end{adjustwidth}

\citeauthor{Schiemer2005pocketgamelan}'s ``Pocket Gamelan'' is another project that used Pd on mobile devices allowing users to compile patches to the Java 2 Platform Micro Edition (J2ME) --- a Java version available to the very first smartphones.
Taking advantage of pd2j2me application, composers could adapt Pd patches to mobile devices without writing any Java code.
This project also created performances using Bluetooth in order to interact with other devices running the same application.

The development of libpd was an important advance in recent years.
This library allows loading of Pd patches inside programs written in Java, Objective C, and other languages.
This solution encouraged the use of Pd on Android and iOS applications.
Examples of these applications are, for example, RjDj and PdDroidParty~\citep{Brinkmann2011embeddingpd,Brinkmann2012makingmusicalapps}.
During our research we also created apps using libpd, like the \textit{thereminimal}, \textit{Hoketus}, and \textit{Sensors2Pd}, described in Appendices~\ref{apesec:appthereminimal}, \ref{apesec:apphoketus}, and \ref{apesubsec:appsensors2pd}, respectively.

``WebPd'' from \citeauthor{Piquemal2017webpd} is a recent project that allows the use of Pd patches on diverse systems through a browser~\citep{Piquemal2017webpd}.
It is a JavaScript library that interprets some Pd elements from the patch, projects them on web pages, and runs the patch using the Web Audio API.
This project is an alternative to those who want to perform using web browsers and Pd without dealing directly with Web Audio API.

\paragraphdesc{port of Cook's STK to mobile into Essl's shamus and urMus}
Based on the ``Synthesis Toolkit'', also known as STK~\citep{Cook1999stk}, \citeauthor{Essl2006mobilestk} developed the ``Mobile STK''~\citep{Essl2006mobilestk}, which was first used on devices running  Symbian OS during the development of the CaMus application~\citep{Rohs2006camus}.
The application was used at interactive musical performances in which visual markers could be detected from the device's camera allowing the control of musical parameters while sending MIDI data through Bluetooth.
CaMus 2 included the interaction between the participants during the performance and on screen information over the camera image~\citep{Rohs2007camus2}.

The Mobile STK was also used on ShaMus~\citep{Essl2007shamus}.
The main difference here is the interaction using new sensors available on mobile devices.
The accelerometer and magnetometer were somewhat imprecise then so the researchers attached an external device to the smartphone for accuracy regarding the mobile device position.
In subsequent research, the audio input was added to the system allowing onset detection and interaction based on the audio amplitude~\citep{Misra2008microphone}. 
The STK was also used as part of the MoMu toolkit and the urMus mobile music environment. 

The Mobile Music Toolkit was the first port of the STK to iOS, motivated by the conception of the iPhone 3G by Apple and the advantages of using the iOS SDK over the Symbian SDK previously used by the authors for their applications~\citep{Bryan2010momu}.
Their justification was that the former SDK provided tools for agile project development while the latter was hard to use for new developers.
Although the MoMu was heavily used by students from Stanford University during the time of its conception as described by the authors, its last update was in 2010.

In turn, the urMus is a mobile application that can be used to create other mobile music applications using Android or iOS devices and a browser.
The users can program interactions and interfaces using the Lua language for their applications and accessing the mobile device from the browser before loading the code to the urMus.
Network interaction is also allowed through Bonjour or TCP connections. 
The system permits the use of all available sensors and input/output interfaces like audio, camera, and GPS as well.
Since its development, urMus has been used in many research projects like a machine learning environment for percussive music collaborations~\citep{Derbinsky2012exploring}, and audience participation using the Mobile Ad-hoc Network~(MANET)~\citep{Lee2014manet}.
Many urMus applications were also created by students as their final project of the ``Building a Mobile Phone Ensemble'' course from the University of Michigan since 2009 under the direction of Prof. Georg Essl and resulted in some performances, the last one being in 2015~\citep{Michigan2017mobilephoneensempleperformance}.

\paragraphdesc{mobile music examples with Chuck}
One of the most prominent MM applications is the Ocarina for iPhone~\citep{Wang2008domobilephones}.
The ChunK language~\citep{wang2003chuck} is embedded in this application to provide audio processing functionalities.
User interaction is made through the blowing on the smartphone's mic while the musical notes are defined by four circles on the screen simulating roles that can be closed by touching the correspondent circle, as in a physical ocarina.
The application was on the Hall of Fame of Apple Store for many years and today users can still play the legendary ocarina on iPhone devices and listen to other users playing around the world~\citep{Wang2014ocarina}. 

\paragraphdesc{mobile music examples with CSound}
During the 2010's, mobile devices became also compatible with one of the oldest computer music languages, the CSound~\citep{Vercoe1990csound}.
Available for Android and iOS devices, many applications have been developed using the mobile API and are distributed as demonstrations inside the CSound mobile application.
During our research, we also used the CSound mobile API to create an application named ``Touches on the Line'' in which user's touch movements are shared through a webserver to all connected users so that they can listen and play together.
The audio synthesis is made with pure CSound while the server uses Ruby on Rails (RoR).
The application is better described in Appendix~\ref{apesec:apptouchesontheline}.

CSound files can also be loaded on to web browsers through the use of the Emscripten compiler that compiles C/C++ into JavaScript applications compatible with Web Audio API, and also through the use of the Portable NativeClient~(PNaCl) that compiles C/C++ into abstract architecture-independent executables.
This web interface is compatible only with specific browsers and is better described by \cite{Lazzarini2014csound}.

\paragraphdesc{mobile music examples with SuperCollider}
Mobile applications using the language ``SuperCollider''~\citep{Mccartney2002supercollider} have recently been conceived and can be found in some research projects as well.
The AuRal system is an example of an Android application that allows users to create ad-hoc ensembles based on geolocation.
The SuperCollider server running on the mobile application receives OSC messages with parameters used during real-time music generation~\citep{Allison2012aural}.
The iSuperColliderKit is an alternative for iOS programmers.
This project provides an embedded version of the SuperCollider server and some interfaces for communication through the Objective C and Swift languages.
SuperCollider code fragments can be sent to the server through native language methods and are interpreted and synthesized afterwards~\citep{Ito2015isupercolliderkit}.

\paragraphdesc{mobile phone orchestras}
The idea of using mobile phones as musical instruments goes far beyond simply creating applications to be used during performances.
The first mobile phone orchestra was conceived in 2007
% by \citefullauthor{Wang2008domobilephones} 
at Stanford University and is known as MoPhO~\citep{Wang2008domobilephones}.
In the beginning, they used the Nokia N95, a revolutionary smartphone from that time in terms of functionalities, available buttons, and the number of sensors.
Later they changed to iPhone devices due to iOS SDK advantages already explained in this section.
The musicians amplified the sound of smartphones with speakers attached to the arms and plugged to the mobile device.
The interaction involved moving the device around, touching its screen, pressing the buttons, or using resources like microphones, speaker feedback, camera, sensors, and also communication technologies like Bluetooth and WiFi.
Siblings of the MoPhO were then created by its idealizers.
Georg Essl conducted the Michigan Mobile Phone Orchestra at the University of Michigan, while the Helsinki Mobile Phone Orchestra was directed by Henri Penttinen at the Helsinki University of Technology in Finland.
This movement inspired the conception of new orchestras at other US universities and countries like South Korea and Japan.

\paragraphdesc{Lee's echobo}
A reference application that was used to propose a collaborative performance with audiences as musicians was the ``echobo'' from \citeauthor{Lee2013echobo}.
This mobile instrument had an audio engine based on the MoMu Toolkit, the user interface created on COCOS 2D API, and the server programed in PHP language. 
The user would interact during a performance through a piano-like interface using the touchscreen of an iOS device.
A musician guided the harmony of the performance by sending chords information to all devices connected to the same room on the application.
One of the performances included more than 100 participants and also a traditional musician playing a clarinet on stage~\citep{Lee2013echobo}.

\paragraphdesc{Crowd in C}
During our research, in partnership with Sang Won Lee and Georg Essl, we conceived another application for audience participation named ``Crowd in C[loud]''.
In this case we took advantage of Web Audio API to create an application that could be accessed from web browsers, avoiding any installation process and presenting compatibility with most mobile's devices OSs.
A musician would also guide the audience harmony from the stage in this case, but, differently from echobo, the devices were interconnected using Cloud Services.
Doing this, we allowed users from anywhere to join the performance without any network setup besides entering a web page URL.
The scalability of Cloud Services is another advantage of this approach that permits a high number of users to interact together, while the distributed Cloud clusters shorten the interconnection paths depending on each user's location.
More details regarding this project are presented in Appendix~\ref{apesec:appcrowdincloud}.

\paragraphdesc{collaboration: controllers merge with instruments}
These selected applications represent only particular cases of MM technologies.
The fact that some of these applications are aimed at interaction between users implied the use of network technologies in most of the situations to allow data exchange among devices.
In the literature, there are also works with mobile controllers interacting with central sound systems through the network~\citep{Weitzner2012massmobile,Allison2013nexus,Hindle2013swarmed}.
Considering the applications cited in this section which used any kind of network, we have the communication concept adhered to other two principles: interaction and collaboration.
Although the communication structure is preceded by decisions regarding technologies, settings, and set ups, these principles are better related to an artistic strategy.
These three keys together deserve some attention for the observations regarding MM instruments.






%% ------------------------------------------------------------------------- %%
\section{Communication: Interaction and Collaboration}
\label{sec:communication}

\paragraphdesc{importance of mobile communication and user interaction}
The term `mobile' in MM is related to the idea of being able to move.
In this case, the music of the musician is moving from one place to another during the music timeline.
The movement is inherent to communication technologies from the computer networks field.
This field improved interaction paradigms by permitting user's intercommunication with network servers, computer machines, and distant users.

\paragraphdesc{data transmission channels and noise}
Users may have different constraints in MM interactions depending on the channel and noise during communication.
%The Intranet through a home router is expected to be a channel with less noise than the Internet through 4G technology, while the latter presents a bigger structure with the throughput reaching hundreds of gigabits per second on some paths and the Intranet offers communication at around 1 gigabit per second.
%Another important point is the distance that the data will travel through in the selected channel.
%Both Intranet and Internet allow short and long range interconnection with unpredictable probability of noise in most situations.
%Data loss and delay can be unacceptable for some MM experiences, although it is not a general rule. 
\paragraphdesc{synchronization problems: example of a simple call}
Data exchange constraints can also affect the idea of synchronization during collaborative MM experiences.
Some works already discussed in Section~\ref{sec:networkmusic} determined that network interaction have problems with delays higher than 30~ms~\cite{Lago2004thequest}.
Imagine a single call between two extremes of the Equator through an optical fiber.
The data would be transmitted at 200,000~km/s over 20,000~km, and the communication would have 100~ms of delay, which is unacceptable by the telephony industry~\citep{Cheshire1996latency}.
Another example is a call using optical fiber between Brazilian extremes, North-South or East-West, through a path of approximately 4,000~km.
The delay in this case would be 20~ms, which is almost acceptable.
The transmission and propagation contributors are considered in these examples, but there are many others, and the final delay clearly would be longer than expected from our calculations~\citep[p.~8831]{Rottondi2016overview}.

%\paragraphdesc{solutions for audience participation during musical performance}

\paragraphdesc{performance models from Marcio Masaki Tomiyoshi's master thesis}
Carot's approaches cited early in this chapter are related to the way users could manage the network constraints and finally interact throughout a performance.
These approaches can also be applied to MM projects that focus on collaboration through any network.
The Realistic Interaction approach considers musicians sharing the same physical space.
This model is close to reality as the data transmission is similar to the sound diffusion during a real performance on a stage.
SWARMED and NEXUS projects are examples of MM experiences following this model.
At SWARMED~\citep{Hindle2013swarmed}, \citefullauthor{Hindle2013swarmed} used a captive portal to captivate participants' attention as they would have restricted Internet access through the portal and would be concentrated on the performance.
The NEXUS project focused on distributing unique interfaces to users accessing a webpage on their mobile devices~
\citep{Allison2013nexus}.
In this case, events from a user's interactions were exchanged by a central server based on Ruby on Rails (RoR).
The server was communicating using OSC with a synthesizer made in the MAX/MSP application.

The Master/Slave approach is applied when there is lager latency between participants of an MM performance.
In this case, one of the participants acts as a master, executing their part during the performance without waiting for the slave's responses in real time.
The slave can react to the master's events while the slave's events can only affect future events on the master's side from the slave's perspective.
CloudOrch, TweetDreams, and Crowd in C[loud] are some examples of this approach as the interaction acts through external servers that provide latencies higher than 25~ms in practice.
The CloudOrch was proposed by \citefullauthor{Hindle2014cloudorch} and deployed virtual machines for client and server users.
In this project, the audio would be streamed from cloud instruments to both desktop computers and mobile devices.
While the cloud server would act as a master sound card, the devices would act as slaves using web browsers and websockets for intercommunication~\citep{Hindle2014cloudorch}.
At TweetDreams, the users would contribute to the main performance through the Tweeter system by sending specified search terms translated into musical and visual reactions~\citep{Dahl2011tweetdreams}.
Python, Chuck, and Processing languages are used in this project to retrieve tweets, generate melodies, and render the graphical interface, respectively.
The Python application acts as the master application, while the slave is the Tweeter application.
Crowd in C[loud], described in Appendix~\ref{apesec:appcrowdincloud}, is a performance where the master stays on stage while the slaves are the audience's mobile devices acting as performers~\citep{Lee2016crowd,deCarvalhoJunior2016understanding}.

In the Delayed Feedback approach Carot defines that, independently of the delay, all inputs will be in sync in the future and the participants will feel like they are working together.
massMobile is an example of this approach as the authors take advantage of time-stamped messages in order to minimize the effects of latency, playing all events together based on the time-stamps.
This project also takes advantage of a robust cloud based Java server and has been used in several performances as a client-server alternative for audience participation while accommodating the unpredictable latency common in MM~\citep{Weitzner2012massmobile}. 

In the Fake Time approach, the issue with latency is avoided.
The musical events will only be played at the next musical bar and the participants will play with events during the previous bar by other participants.
The project `Touches On The Line' described in Appendix~\ref{apesec:apptouchesontheline} can be considered an example of this approach as the finger movements on touchscreen are sent only when the finger releases the screen~\cite{deCarvalhoJunior2013touches}.
Although the application was conceived for distributed performances, when all devices share the same physical space, the musicians can hear what will be played in the future and then react consciously, as for example during my performance at the 2nd International CSound Conference. 

The Latency Accepting approach is applied in cases when the composer and musicians do accept latency and packet loss as part of the performance.
This model of collaboration is more relaxed regarding network constraints.
The application Sensors2OSC described in Appendix~\ref{apesubsec:appsensors2osc} is an example of this approach~\citep{deCarvalhoJunior2015sensors2osc}.
The data is exchanged within this application context using the UDP protocol that doesn't guarantee packet delivery and can have a high latency depending on the network.

\paragraphdesc{Internet Music discussion}
All of these approaches are deeply related to the internet music concept from the perspective of those who want to deal with these problems.
The technological and physical restrictions do exist and are unavoidable in some situations, such as long-distance interaction.
However, depending on the technical requirement, specific technologies will be better for intercommunication in short or long ranges, in order to leverage the collaborative proposal.
Some technologies are briefly described below, while their technical settings are presented in the next chapter.

\paragraphdesc{technologies as solution for local communication: infrared, bluetooth, WiFi}
Considering short range intercommunication for MM, we can take some specific technologies into account, such as infrared, Bluetooth, and WiFi.
The first is rarely available on mobile devices but is one of the cheapest interfaces for communication.
Although it requires a close face-to-face interaction, the data exchange is simply pointing one device at another.
Bluetooth technology permits wireless communication with close devices in a range from 10 to 100 square meters and the devices are free to move even during the connection.
WiFi is the commonest of these three and the communication range can exceed hundreds of meters with local connections available in open places, which is sufficient for short range situations.
In this case, the compatibility depends on the WiFi standards available at the router. 
Although current devices can reach gigabits per second of data transfer using 802.11~ac, megabits per second solutions are more common and compatible nowadays.

\paragraphdesc{technologies as solution for long distance communication: WiFi, 4G, cloud}
Wifi and also 4G are good solutions when long range communication is required.
The main advantage of 4G devices is their backward compatibility with 3G, and both 2G technologies EDGE and GPRS.
On the other hand, the 4G signal can be reduced or unavailable in indoor situations depending on the antenna's location.
Considering the antennas, the ability of keeping the connection on movement is achieved by switching between antennas for WiFi through the use of repeaters and with a handover process to 4G.

\paragraphdesc{discussion about scalability and mobile networks}
Internet access may be required for short or long range communications during collaborative practices.
Infrared, Bluetooth, and WiFi permit access to the Internet through the use of other devices such as switch or access point, while the 4G technology offers Internet from the service provider.
A common approach can be the use of servers in order to interconnect the devices, and the advances in Cloud Computing are encouraging its use.
Cloud solutions can provide features like scalability and also different levels of services such as infrastructure, platform, software, monitoring, communication, or even anything as a service.

From the perspective of internet music, once connected to the Internet, a device can interconnect with an unlimited number of devices.
Musicians and users of these technologies are mostly unaware of their technical details and constraints.
With this in mind, a more detailed discussion regarding the technical settings of MM networking technologies is provided in the next chapter.